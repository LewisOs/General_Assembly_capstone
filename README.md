# How much is that doggy in my (browser) window?

This is the capstone project from my Data Science Immersive course at General Assembly, which I completed on 03/02/23. Whilst the project itself is complete, this README file is a **work in progress**. Please bare with me whilst I update it.

## Intro

During the COVID-19 pandemic, the number of British households buying pets increased massively. This caused the prices of many pets (particularly dogs) to rise significantly (with prices for some breeds more than doubling). More recently, the cost of living crisis has made taking on a new pet financially prohibitive for many households. Following these ups and downs in the demand for pets, it is difficult for both sellers and buyers to know what is a fair price (or at least the market rate) for different animal types. Given this, I wanted to see if it would be possible to build a model which could accurately predict the prices of pets based on their attributes. I decided to treat this as a regression problem (rather than binning pets in price bands to classify them) and to set a target of attaining an R2 of 0.80.

## Data collection

In order to base my model on up to date data about pet prices, I decided I would build a data set by scraping information about pet adverts off of an online pet sales platform. Specifically; Pets4Homes.co.uk. I selected this website as it is among the most popular (if not the most) pet sales websites in the UK. It also allows sellers to list a large amount of information about the pets they are selling. I approached the scrape in two stages. In stage one, I used Selenium to search for particular types of pets and to navigate through the results pages. Each page of results returned around twenty listings and provided a brief overview of each listing. By converting the page of results into a Beautiful Soup object, I was able to extract the URL for the full advert associated with each listing. I ran this scrape until I had collected over 20,000 unique URLs. I had initially tried to use Python's Requests library for this process. However, Pets4Homes.co.uk immediately blocked this (even when I tried methods such has using a VPN, or manually changing my headers). 

The second stage of the scrape involved going to each of the URLs and extracting the relevant information about the listing. As before, I did this with a combination of Selenium and Beautiful Soup. However, I encountered a problem at this stage. Pets4Homes.co.uk dynamically update the class names used for the HTML elements on their website. As my first scrape was relatively quick, I did not intially notice this. But since my 2nd scrape involved around 20x more webpages and was done in stages, it became apparent that the class names were changing multiple times a day. As such, the code I had initially written pull information about the listings out kept becoming obselete. I was however able to find a work around for this. Rather than only scraping the information I needed, I simply scraped the full HTML for each of the 20,000+ pages, meaning I had static version of each page's HTML. Unfortunately, as the classes changed several times during this process, I still had several larger chunks of listings with different class names. I had only mitigated the problem, rather than eliminating it completely. In the end, I worked out how to extract almost all of the information I was interested in from the HTML without using the class names. For instance, I was able to determine if a seller was verified by checking the colour of the associated icon. With this problem solved...  


